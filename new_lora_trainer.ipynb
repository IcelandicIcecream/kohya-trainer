{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gS9wNcX-E06s"
      ],
      "authorship_tag": "ABX9TyMixTQMqiMQHf/pGQVbS4Ed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IcelandicIcecream/kohya-trainer/blob/main/new_lora_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kohya Trainer\n",
        "#### Bootstrapped from https://github.com/Linaqruf/kohya-trainer, removed the unnecessary portion and uses improves captioning\n"
      ],
      "metadata": {
        "id": "gS9wNcX-E06s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G9R1_LflnB6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327180d6-a2be-44d4-b272-11c8b3e950ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 23 17:54:56 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Stored 'root_dir' (str)\n",
            "Stored 'repo_dir' (str)\n",
            "Stored 'tools_dir' (str)\n",
            "Stored 'finetune_dir' (str)\n",
            "Stored 'training_dir' (str)\n",
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 1196, done.\u001b[K\n",
            "remote: Total 1196 (delta 0), reused 0 (delta 0), pack-reused 1196\u001b[K\n",
            "Receiving objects: 100% (1196/1196), 2.89 MiB | 5.94 MiB/s, done.\n",
            "Resolving deltas: 100% (742/742), done.\n"
          ]
        }
      ],
      "source": [
        "#@title ## 1.1. Clone Kohya Trainer\n",
        "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.\n",
        "\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "root_dir = \"/content\"\n",
        "%store root_dir\n",
        "repo_dir = str(root_dir)+\"/kohya-trainer\"\n",
        "%store repo_dir\n",
        "tools_dir = str(root_dir)+\"/kohya-trainer/tools\"\n",
        "%store tools_dir \n",
        "finetune_dir = str(root_dir)+\"/kohya-trainer/finetune\"\n",
        "%store finetune_dir\n",
        "training_dir = str(root_dir)+\"/dreambooth\"\n",
        "%store training_dir\n",
        "\n",
        "branch = \"\" #@param {type: \"string\"}\n",
        "repo_url = \"https://github.com/IcelandicIcecream/kohya-trainer.git\"\n",
        "\n",
        "def clone_repo():\n",
        "  if os.path.isdir(repo_dir):\n",
        "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "  else:\n",
        "    %cd {root_dir}\n",
        "    !git clone {repo_url} {repo_dir}\n",
        "\n",
        "if not os.path.isdir(repo_dir):\n",
        "  clone_repo()\n",
        "\n",
        "%cd {root_dir}\n",
        "os.makedirs(repo_dir, exist_ok=True)\n",
        "os.makedirs(tools_dir, exist_ok=True)\n",
        "os.makedirs(finetune_dir, exist_ok=True)\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "\n",
        "if branch:\n",
        "  %cd {repo_dir}\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J2BSarXjEz7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.2. Installing Dependencies\n",
        "#@markdown This will install required Python packages\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "%store accelerate_config\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "\n",
        "def install_dependencies():\n",
        "    !pip -q install --upgrade gallery-dl gdown imjoy-elfinder\n",
        "    !apt -q install liblz4-tool aria2\n",
        "    !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "    if install_xformers:\n",
        "        !pip -q install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "install_dependencies()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mIC0v_reWuyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt9EJv5gQXuB"
      },
      "source": [
        "## 1.3. Sign-in to Cloud Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rl2zERHbBQ9W"
      },
      "outputs": [],
      "source": [
        "#@title ### 1.3.1. Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "%store -r\n",
        "\n",
        "#@markdown Login to Huggingface hub\n",
        "#@markdown 1. You need a Huggingface account.\n",
        "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
        "write_token = \"\" #@param {type:\"string\"}\n",
        "os.environ['TOKEN'] = write_token\n",
        "login(write_token, add_to_git_credential=True)\n",
        "\n",
        "%store write_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKL38-WmQsLN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ### 1.3.2. Mount Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2.1. Download Available Model \n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installModels = []\n",
        "installv2Models = []\n",
        "\n",
        "#@markdown ### Available Model\n",
        "#@markdown Select one of available model to download:\n",
        "\n",
        "#@markdown ### SD1.x model\n",
        "modelUrl = [\"\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\", \\\n",
        "            \"https://huggingface.co/gsdf/Counterfeit-V2.0/resolve/main/Counterfeit-V2.0fp16.safetensors\", \\\n",
        "            \"https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt\", \\\n",
        "            \"https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt\", \\\n",
        "            \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt\", \\\n",
        "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\"]\n",
        "modelList = [\"\", \\\n",
        "             \"Animefull-final-pruned\", \\\n",
        "             \"Anything-v3-1\", \\\n",
        "             \"Anything-v4-5-pruned\", \\\n",
        "             \"Kani-anime-pruned\", \\\n",
        "             \"AbyssOrangeMix2-nsfw\", \\\n",
        "             \"Counterfeit-v2\", \\\n",
        "             \"DpepTeaHands3\", \\\n",
        "             \"OpenJourney-v2\", \\\n",
        "             \"Dreamlike-diffusion-v1-0\", \\\n",
        "             \"Stable-Diffusion-v1-5\"]\n",
        "modelName = \"Stable-Diffusion-v1-5\"  #@param [\"\", \"Animefull-final-pruned\", \"Anything-v3-1\", \"Anything-v4-5-pruned\", \"Kani-anime-pruned\", \"AbyssOrangeMix2-nsfw\", \"Counterfeit-v2\", \"DpepTeaHands3\", \"OpenJourney-v2\", \"Dreamlike-diffusion-v1-0\", \"Stable-Diffusion-v1-5\"]\n",
        "\n",
        "#@markdown ### SD2.x model\n",
        "v2ModelUrl = [\"\", \\\n",
        "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt\", \\\n",
        "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.ckpt\", \\\n",
        "              \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt\", \\\n",
        "              \"https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors\"]\n",
        "v2ModelList = [\"\", \\\n",
        "              \"stable-diffusion-2-1-base\", \\\n",
        "              \"stable-diffusion-2-1-768v\", \\\n",
        "              \"waifu-diffusion-1-4-anime-e2\", \\\n",
        "              \"plat-diffusion-v1-3-1\"]\n",
        "v2ModelName = \"\" #@param [\"\", \"stable-diffusion-2-1-base\", \"stable-diffusion-2-1-768v\", \"waifu-diffusion-1-4-anime-e2\", \"plat-diffusion-v1-3-1\"]\n",
        "\n",
        "if modelName != \"\":\n",
        "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "if v2ModelName != \"\":\n",
        "  installv2Models.append((v2ModelName, v2ModelUrl[v2ModelList.index(v2ModelName)]))\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
        "\n",
        "  hf_token = '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' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir}/pre_trained_model -o {checkpoint_name}.{ext} \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "  for v2model in installv2Models:\n",
        "    install(v2model[0], v2model[1])\n",
        "\n",
        "install_checkpoint()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "naylZp9Lyz3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3LWn6GzNQ4j5"
      },
      "outputs": [],
      "source": [
        "#@title ## 2.2. Download Custom Model\n",
        "\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "#@markdown ### Custom model\n",
        "modelUrl = \"\" #@param {'type': 'string'}\n",
        "dst = str(root_dir)+\"/pre_trained_model\"\n",
        "\n",
        "if not os.path.exists(dst):\n",
        "    os.makedirs(dst)\n",
        "\n",
        "def install(url):\n",
        "  base_name = os.path.basename(url)\n",
        "\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    %cd {dst}\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    #@markdown Change this part with your own huggingface token if you need to download your private model\n",
        "    hf_token = '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' #@param {type:\"string\"}\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dst} -o {base_name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dst} -o {url}\n",
        "\n",
        "install(modelUrl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SoucgZQ6jgPQ"
      },
      "outputs": [],
      "source": [
        "#@title ## 2.3. Download Available VAE (Optional)\n",
        "%store -r \n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installVae = []\n",
        "#@markdown ### Available VAE\n",
        "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\", \\\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"stablediffusion.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "def install(vae_name, url):\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} \"{url}\"\n",
        "\n",
        "def install_vae():\n",
        "  if vaeName != \"none\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "install_vae()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En9UUwGNMRMM"
      },
      "source": [
        "# Data Acquisition & Processing\n",
        "\n",
        "Take files from google drive and create the folder structure for LoRA Training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh7CeDqK4l3Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ## 3.1. Define Train Data Directory\n",
        "#@markdown Define the location of your training data. This cell will also create a folder based on your input.\n",
        "#@markdown The parent directory set here will be used to create the folder structure for the training.\n",
        "%store -r\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "#uploading to drive is way faster than uploading in colab.\n",
        "drive_directory = \"/content/drive/MyDrive/dreambooth/sample_photos/ming_friend\" #@param {type:\"string\"}\n",
        "drive_content = os.listdir(drive_directory)\n",
        "%store drive_directory\n",
        "%store drive_content\n",
        "\n",
        "#this is the directory where you want the training data to be stored\n",
        "parent_directory = \"/content/dreambooth/train_data/minga7\" #@param {type: \"string\"}\n",
        "%store parent_directory\n",
        "\n",
        "#no idea what this is\n",
        "reg_folder_directory = os.path.join(os.path.dirname(parent_directory), \"reg_data\")\n",
        "%store reg_folder_directory\n",
        "\n",
        "#the kohya trainer sets the number of repeats based on the folder name,\n",
        "#it's recommended that a pictures get at least 100 repeats, there should be a minimum of 1500 steps whatever the number of pictures.\n",
        "\n",
        "calculate_repeats = True #@param {type: \"boolean\"}\n",
        "#@markdown Train_repeats will be calculated if checked based on your photos, if not you can set it below. here will be ignored if calculate_repeats is checked.\n",
        "photo_qty = len(drive_content)\n",
        "if calculate_repeats:\n",
        "  if photo_qty*100 < 1500:\n",
        "    train_repeats = int(math.ceil(1500/photo_qty))\n",
        "  else:\n",
        "    train_repeats = 100\n",
        "else:\n",
        "  train_repeats = 30 #@param {type: \"integer\"}\n",
        "\n",
        "reg_repeats = 1 #@param {type: \"integer\"}\n",
        "concept_name = \"minga7\" #@param {type: \"string\"}\n",
        "class_name = \"\" #@param {type: \"string\"}\n",
        "#@markdown You can run this cell multiple time to add new concepts\n",
        "\n",
        "def get_folder_name(repeats, class_name, concept_name=None):\n",
        "  if class_name:\n",
        "    return f\"{repeats}_{concept_name} {class_name}\" if concept_name else f\"{repeats}_{class_name}\"\n",
        "  return f\"{repeats}_{concept_name}\"\n",
        "\n",
        "train_folder = get_folder_name(train_repeats, class_name, concept_name=concept_name)\n",
        "reg_folder = get_folder_name(reg_repeats, class_name)\n",
        "\n",
        "train_data_dir = os.path.join(parent_directory, train_folder)\n",
        "reg_data_dir = os.path.join(reg_folder_directory, reg_folder)\n",
        "\n",
        "os.makedirs(parent_directory, exist_ok=True)\n",
        "os.makedirs(reg_folder_directory, exist_ok=True)\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "os.makedirs(reg_data_dir, exist_ok=True)\n",
        "\n",
        "for files in drive_content:\n",
        "  shutil.copy(f\"{drive_directory}/{files}\", train_data_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2 Generate Captions with BLIP\n",
        "%cd {root_dir}\n",
        "!git clone https://github.com/salesforce/BLIP\n",
        "%cd BLIP\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from google.colab import files\n",
        "from models.blip import blip_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "]) \n",
        "\n",
        "images = []\n",
        "image_size = 768 #@param {type: \"integer\"}\n",
        "num_beams = 3 #@param {type: \"integer\"}\n",
        "max_length = 100 #@param {type: \"integer\"}\n",
        "min_length = 5 #@param {type: \"integer\"}\n",
        "\n",
        "for file_name in drive_content:\n",
        "  image_directory = f\"{drive_directory}/{file_name}\"\n",
        "  raw_image = Image.open(image_directory).convert('RGB')\n",
        "  image = [file_name, transform(raw_image).unsqueeze(0).to(device)]\n",
        "  images.append(image)\n",
        "\n",
        "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "%cd {train_data_dir} \n",
        "\n",
        "with torch.no_grad():\n",
        "  print(f\"\\nimage_size = {image_size}, num_beams = {num_beams}, max_length = {max_length}, min_length = {min_length}\\n\")\n",
        "  for image in images:\n",
        "    caption = model.generate(image[1], sample=False, num_beams=num_beams, max_length=max_length, min_length=min_length) \n",
        "    with open(str(f\"{image[0]}\").replace(\".jpg\",\".txt\"), \"w\") as file:\n",
        "      file.write(concept_name + \" \" + caption[0])\n",
        "    print(f\"Caption for {image[0]} is '{concept_name} {caption[0]}'\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HJvTSrSuQ23F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.3 Remove Unwanted Words\n",
        "unw_words = [\"with long hair\", \"with long dark hair\", \"with long bright hair\", \"hair\", \" long\", \"short\", \" smiling at the camera\", \" smiling\"]\n",
        "\n",
        "for file_names in os.listdir(train_data_dir):\n",
        "  if \".txt\" in file_names:\n",
        "    caption = open(file_names).read()\n",
        "    for words in unw_words:\n",
        "      changing_caption = caption.replace(words,\"\")\n",
        "      caption = changing_caption.replace(words,\"\")\n",
        "      print(caption)\n",
        "    open(file_names, \"w\").write(caption)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "69rh_Er4iuJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxfAtz2EZXGY",
        "outputId": "cc178071-4e1b-477c-fd12-016282a5c1a2",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored 'train_folder_directory' (str)\n",
            "Stored 'reg_folder_directory' (str)\n",
            "--2023-02-23 19:03:26--  https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/v2-inference-v.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1815 (1.8K) [text/plain]\n",
            "Saving to: ‘/content/drive/MyDrive/dreambooth/output/minga7.yaml’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   1.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-23 19:03:26 (11.9 MB/s) - ‘/content/drive/MyDrive/dreambooth/output/minga7.yaml’ saved [1815/1815]\n",
            "\n",
            "File successfully downloaded\n"
          ]
        }
      ],
      "source": [
        "#@title ## 3.4 Define Important folders\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "v2 = True #@param {type:\"boolean\"}\n",
        "v_parameterization = True #@param {type:\"boolean\"}\n",
        "project_name = \"minga7\" #@param {type:\"string\"}\n",
        "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2-1\" #@param {type:\"string\"}\n",
        "vae = \"/content/vae/stablediffusion.vae.pt\"  #@param {type:\"string\"}\n",
        "#@markdown You need to register parent folder and not where `train_data_dir` located\n",
        "train_folder_directory = \"/content/dreambooth/train_data/minga7\" #@param {'type':'string'}\n",
        "%store train_folder_directory\n",
        "reg_folder_directory = \"/content/dreambooth/train_data/minga7\" #@param {'type':'string'}\n",
        "%store reg_folder_directory\n",
        "output_dir = \"/content/dreambooth/output\" #@param {'type':'string'}\n",
        "resume_path =\"\"\n",
        "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
        "\n",
        "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/fine_tune/output` by default\n",
        "output_to_drive = True #@param {'type':'boolean'}\n",
        "\n",
        "if output_to_drive:\n",
        "  output_dir = \"/content/drive/MyDrive/dreambooth/output\"\n",
        "\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')  \n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "if v2 and not v_parameterization:\n",
        "  inference_url += \"v2-inference.yaml\"\n",
        "if v2 and v_parameterization:\n",
        "  inference_url += \"v2-inference-v.yaml\"\n",
        "\n",
        "try:\n",
        "  if v2:\n",
        "    !wget {inference_url} -O {output_dir}/{project_name}.yaml\n",
        "    print(\"File successfully downloaded\")\n",
        "except:\n",
        "  print(\"There was an error downloading the file. Please check the URL and try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5P-QVvHMUrFB"
      },
      "outputs": [],
      "source": [
        "#@title ## 4.0 Define Specific LoRA Training Parameters\n",
        "%store -r\n",
        "\n",
        "#@markdown ## LoRA - Low Rank Adaptation Dreambooth\n",
        "\n",
        "#@markdown Some people recommend setting the `network_dim` to a higher value.\n",
        "network_dim = 128 #@param {'type':'number'}\n",
        "#@markdown For weight scaling in LoRA, it is better to set `network_alpha` the same as `network_dim` unless you know what you're doing. A lower `network_alpha` requires a higher learning rate. For example, if `network_alpha = 1`, then `unet_lr = 1e-3`.\n",
        "network_alpha = 128 #@param {'type':'number'}\n",
        "network_module = \"networks.lora\"\n",
        "\n",
        "#@markdown `network_weights` can be specified to resume training.\n",
        "network_weights = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown By default, both Text Encoder and U-Net LoRA modules are enabled. Use `network_train_on` to specify which module to train.\n",
        "network_train_on = \"both\" #@param ['both','unet_only', 'text_encoder_only'] {'type':'string'}\n",
        "\n",
        "#@markdown It is recommended to set the `text_encoder_lr` to a lower learning rate, such as `5e-5`, or to set `text_encoder_lr = 1/2 * unet_lr`.\n",
        "learning_rate = 1e-4 #@param {'type':'number'}\n",
        "unet_lr = 1e-4 #@param {'type':'number'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
        "\n",
        "#@markdown If `lr_scheduler = cosine_with_restarts`, update `lr_scheduler_num_cycles`.\n",
        "lr_scheduler_num_cycles = 1 #@param {'type':'number'}\n",
        "#@markdown If `lr_scheduler = polynomial`, update `lr_scheduler_power`.\n",
        "lr_scheduler_power = 1 #@param {'type':'number'}\n",
        "\n",
        "#@markdown Check the box to not save metadata in the output model.\n",
        "no_metadata = False #@param {type:\"boolean\"}\n",
        "training_comment = \"this comment will be stored in the metadata\" #@param {'type':'string'}\n",
        "\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if network_weights == \"\":\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weights):\n",
        "    print(\"Loading LoRA weight:\", network_weights)\n",
        "  else:\n",
        "    print(f\"{network_weights} does not exist.\")\n",
        "    network_weights =\"\"\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enabling LoRA for U-Net.\")\n",
        "  print(\"Disabling LoRA for Text Encoder.\")\n",
        "\n",
        "print(\"Global learning rate: \", learning_rate)\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enable LoRA for U-Net\")\n",
        "  print(\"Disable LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
        "if network_train_on == \"text_encoder_only\":\n",
        "  print(\"Disabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
        "else:\n",
        "  print(\"Enabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
        "\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- Number of cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- Power: \", lr_scheduler_power)\n",
        "\n",
        "# Printing the training comment if metadata is not disabled and a comment is present\n",
        "if not no_metadata:\n",
        "  if training_comment: \n",
        "    training_comment = training_comment.replace(\" \", \"_\")\n",
        "    print(\"Training comment:\", training_comment)\n",
        "else:\n",
        "  print(\"Metadata won't be saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GIvfcS1pOrGY"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "import textwrap\n",
        "import yaml\n",
        "\n",
        "%store -r\n",
        "\n",
        "#@title ## 4.1 Start LoRA Dreambooth\n",
        "#@markdown ### Define Parameter\n",
        "\n",
        "train_batch_size = 1 #@param {type:\"number\"}\n",
        "num_epochs = 1 #@param {type:\"number\"}\n",
        "caption_extension = '.txt' #@param {'type':'string'}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_n_epoch_ratio\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "resolution = 768 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "enable_bucket = True #@param {type:\"boolean\"}\n",
        "min_bucket_reso = 320 if resolution > 640 else 256\n",
        "max_bucket_reso = 1280 if resolution > 640 else 1024\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "use_8bit_adam = True #@param {type:\"boolean\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = 1234 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/dreambooth/logs\"\n",
        "log_prefix = project_name\n",
        "additional_argument = \"--shuffle_caption --xformers\" #@param {type:\"string\"}\n",
        "print_hyperparameter = True #@param {type:\"boolean\"}\n",
        "prior_loss_weight = 1.0\n",
        "%cd {repo_dir}\n",
        "\n",
        "train_command=f\"\"\"\n",
        "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_network.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --network_dim={network_dim} \\\n",
        "  --network_alpha={network_alpha} \\\n",
        "  --network_module={network_module} \\\n",
        "  {\"--network_weights=\" + network_weights if network_weights else \"\"} \\\n",
        "  {\"--network_train_unet_only\" if network_train_on == \"unet_only\" else \"\"} \\\n",
        "  {\"--network_train_text_encoder_only\" if network_train_on == \"text_encoder_only\" else \"\"} \\\n",
        "  --learning_rate={learning_rate} \\\n",
        "  {\"--unet_lr=\" + format(unet_lr) if unet_lr !=0 else \"\"} \\\n",
        "  {\"--text_encoder_lr=\" + format(text_encoder_lr) if text_encoder_lr !=0 else \"\"} \\\n",
        "  {\"--no_metadata\" if no_metadata else \"\"} \\\n",
        "  {\"--training_comment=\" + training_comment if training_comment and not no_metadata else \"\"} \\\n",
        "  --lr_scheduler={lr_scheduler} \\\n",
        "  {\"--lr_scheduler_num_cycles=\" + format(lr_scheduler_num_cycles) if lr_scheduler == \"cosine_with_restarts\" else \"\"} \\\n",
        "  {\"--lr_scheduler_power=\" + format(lr_scheduler_power) if lr_scheduler == \"polynomial\" else \"\"} \\\n",
        "  --pretrained_model_name_or_path={pretrained_model_name_or_path} \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  {\"--caption_extension=\" + caption_extension if caption_extension else \"\"} \\\n",
        "  --train_data_dir={train_folder_directory} \\\n",
        "  --reg_data_dir={reg_folder_directory} \\\n",
        "  --output_dir={output_dir} \\\n",
        "  --prior_loss_weight={prior_loss_weight} \\\n",
        "  {\"--resume=\" + resume_path if resume_path else \"\"} \\\n",
        "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
        "  --mixed_precision={mixed_precision} \\\n",
        "  --save_precision={save_precision} \\\n",
        "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
        "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
        "  --save_model_as={save_model_as} \\\n",
        "  --resolution={resolution} \\\n",
        "  {\"--enable_bucket\" if enable_bucket else \"\"} \\\n",
        "  {\"--min_bucket_reso=\" + format(min_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--max_bucket_reso=\" + format(max_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--cache_latents\" if cache_latents else \"\"} \\\n",
        "  --train_batch_size={train_batch_size} \\\n",
        "  --max_token_length={max_token_length} \\\n",
        "  {\"--use_8bit_adam\" if use_8bit_adam else \"\"} \\\n",
        "  --max_train_epochs={num_epochs} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  {\"--gradient_checkpointing\" if gradient_checkpointing else \"\"} \\\n",
        "  {\"--gradient_accumulation_steps=\" + format(gradient_accumulation_steps) } \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --logging_dir={logging_dir} \\\n",
        "  --log_prefix={log_prefix} \\\n",
        "  {additional_argument}\n",
        "  \"\"\"\n",
        "\n",
        "debug_params = [\"v2\", \\\n",
        "                \"v_parameterization\", \\\n",
        "                \"network_dim\", \\\n",
        "                \"network_alpha\", \\\n",
        "                \"network_module\", \\\n",
        "                \"network_weights\", \\\n",
        "                \"network_train_on\", \\\n",
        "                \"learning_rate\", \\\n",
        "                \"unet_lr\", \\\n",
        "                \"text_encoder_lr\", \\\n",
        "                \"no_metadata\", \\\n",
        "                \"training_comment\", \\\n",
        "                \"lr_scheduler\", \\\n",
        "                \"lr_scheduler_num_cycles\", \\\n",
        "                \"lr_scheduler_power\", \\\n",
        "                \"pretrained_model_name_or_path\", \\\n",
        "                \"vae\", \\\n",
        "                \"caption_extension\", \\\n",
        "                \"train_folder_directory\", \\\n",
        "                \"reg_folder_directory\", \\\n",
        "                \"output_dir\", \\\n",
        "                \"prior_loss_weight\", \\\n",
        "                \"resume_path\", \\\n",
        "                \"project_name\", \\\n",
        "                \"mixed_precision\", \\\n",
        "                \"save_precision\", \\\n",
        "                \"save_n_epochs_type\", \\\n",
        "                \"save_n_epochs_type_value\", \\\n",
        "                \"save_model_as\", \\\n",
        "                \"resolution\", \\\n",
        "                \"enable_bucket\", \\\n",
        "                \"min_bucket_reso\", \\\n",
        "                \"max_bucket_reso\", \\\n",
        "                \"cache_latents\", \\\n",
        "                \"train_batch_size\", \\\n",
        "                \"max_token_length\", \\\n",
        "                \"use_8bit_adam\", \\\n",
        "                \"num_epochs\", \\\n",
        "                \"seed\", \\\n",
        "                \"gradient_checkpointing\", \\\n",
        "                \"gradient_accumulation_steps\", \\\n",
        "                \"clip_skip\", \\\n",
        "                \"logging_dir\", \\\n",
        "                \"log_prefix\", \\\n",
        "                \"additional_argument\"]\n",
        "\n",
        "if print_hyperparameter:\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Hyperparameter\", \"Value\"]\n",
        "    for params in debug_params:\n",
        "        if params != \"\":\n",
        "            if globals()[params] == \"\":\n",
        "                value = \"False\"\n",
        "            else:\n",
        "                value = globals()[params]\n",
        "            table.add_row([params, value])\n",
        "    table.align = \"l\"\n",
        "    print(table)\n",
        "\n",
        "    arg_list = train_command.split()\n",
        "    mod_train_command = {'command': arg_list}\n",
        "    \n",
        "    train_folder = os.path.dirname(output_dir)\n",
        "    \n",
        "    # save the YAML string to a file\n",
        "    with open(str(train_folder)+'/dreambooth_lora_cmd.yaml', 'w') as f:\n",
        "        yaml.dump(mod_train_command, f)\n",
        "\n",
        "f = open(\"./train.sh\", \"w\")\n",
        "f.write(train_command)\n",
        "f.close()\n",
        "!chmod +x ./train.sh\n",
        "!./train.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.0 Inference\n",
        "%store -r\n",
        "\n",
        "#@markdown LoRA Config\n",
        "network_weights = \"/content/drive/MyDrive/dreambooth/output/minga7.safetensors\" #@param {'type':'string'}\n",
        "network_module = \"networks.lora\"\n",
        "network_mul = 1 #@param {'type':'number'}\n",
        "\n",
        "#@markdown Other Config\n",
        "v2 = True #@param {type:\"boolean\"}\n",
        "v_parameterization = True #@param {type:\"boolean\"}\n",
        "instance_prompt = \"a photo of minga7\" #@param {type: \"string\"}\n",
        "prompt = \"masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\" #@param {type: \"string\"}\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"}\n",
        "model = \"stabilityai/stable-diffusion-2-1\" #@param {type: \"string\"}\n",
        "vae = \"/content/vae/stablediffusion.vae.pt\" #@param {type: \"string\"}\n",
        "outdir = \"/content/drive/MyDrive/dreambooth/output/generated\" #@param {type: \"string\"}\n",
        "scale = 7 #@param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 50 #@param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 512 #@param {type: \"integer\"}\n",
        "height = 768 #@param {type: \"integer\"}\n",
        "images_per_prompt = 4 #@param {type: \"integer\"}\n",
        "batch_size = 4 #@param {type: \"integer\"}\n",
        "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 40}\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "\n",
        "final_prompt = f\"{instance_prompt}, {prompt} --n {negative}\" if instance_prompt else f\"{prompt} --n {negative}\"\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "!python gen_img_diffusers.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --network_module={network_module} \\\n",
        "  --network_weight={network_weights} \\\n",
        "  --network_mul={network_mul} \\\n",
        "  --ckpt={model} \\\n",
        "  --outdir={outdir} \\\n",
        "  --xformers \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  --{precision} \\\n",
        "  --W={width} \\\n",
        "  --H={height} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  --scale={scale} \\\n",
        "  --sampler={sampler} \\\n",
        "  --steps={steps} \\\n",
        "  --max_embeddings_multiples=3 \\\n",
        "  --batch_size={batch_size} \\\n",
        "  --images_per_prompt={images_per_prompt} \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --prompt=\"{final_prompt}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "FKBrTDPrcNjP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rLdEpPKTbI1I"
      },
      "outputs": [],
      "source": [
        "#@title ## 6.0 Compressing model or dataset\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "zip_module = \"zipfile\" #@param [\"zipfile\", \"shutil\", \"pyminizip\", \"zip\"]\n",
        "directory_to_zip = '/content/dreambooth/train_data/minga7/150_minga7' #@param {type: \"string\"}\n",
        "output_filename = '/content/drive/MyDrive/dreambooth/minga7dataset.zip' #@param {type: \"string\"}\n",
        "password = \"\" #@param {type: \"string\"}\n",
        "\n",
        "if zip_module == \"zipfile\":\n",
        "    with zipfile.ZipFile(output_filename, 'w') as zip:\n",
        "        for directory_to_zip, dirs, files in os.walk(directory_to_zip):\n",
        "            for file in files:\n",
        "                zip.write(os.path.join(directory_to_zip, file))\n",
        "elif zip_module == \"shutil\":\n",
        "    shutil.make_archive(output_filename, 'zip', directory_to_zip)\n",
        "elif zip_module == \"pyminizip\":\n",
        "    !pip install pyminizip\n",
        "    import pyminizip\n",
        "    for root, dirs, files in os.walk(directory_to_zip):\n",
        "        for file in files:\n",
        "            pyminizip.compress(os.path.join(root, file), \"\", os.path.join(\"*\",output_filename), password, 5)\n",
        "elif zip_module == \"zip\":\n",
        "    !zip -rv -q -j {output_filename} {directory_to_zip}"
      ]
    }
  ]
}